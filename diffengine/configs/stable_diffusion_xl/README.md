# Stable Diffusion XL

[Stable Diffusion XL](https://arxiv.org/abs/2307.01952)

## Abstract

We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators.

<div align=center>
<img src="https://github.com/okotaku/diffengine/assets/24734142/27d4ebad-5705-4500-826f-41f425a08c0d"/>
</div>

## Citation

```
```

## Run Training

Run Training

```
# single gpu
$ diffengine train ${CONFIG_FILE}
# multi gpus
$ NPROC_PER_NODE=${GPU_NUM} diffengine train ${CONFIG_FILE}

# Example.
$ diffengine train stable_diffusion_v15_pokemon_blip
```

## Training Speed

#### Single GPU

Environment:

- A6000 Single GPU
- nvcr.io/nvidia/pytorch:24.01-py3

Settings:

- For the final result, we consider the second epoch time due to warm-up issues.
- No gradient accumulation.
- 833 samples. Batch size = 1.

|                    Model                     | time per 1epoch |
| :------------------------------------------: | :-------------: |
|   stable_diffusion_xl_pokemon_blip (BF16)    |    10 m 46 s    |
|   stable_diffusion_xl_pokemon_blip_compile   |    9 m 57 s     |
|    stable_diffusion_xl_pokemon_blip_sfast    |    7 m 49 s     |
| stable_diffusion_xl_pokemon_blip_pre_compute |    7 m 37 s     |

Note that `stable_diffusion_xl_pokemon_blip_compile` took a few minutes to compile.

## Inference with diffusers

Once you have trained a model, specify the path to the saved model and utilize it for inference using the `diffusers.pipeline` module.

```py
import torch
from diffusers import DiffusionPipeline, UNet2DConditionModel, AutoencoderKL

prompt = 'yoda pokemon'
checkpoint = 'work_dirs/stable_diffusion_xl_pokemon_blip/step41650'

unet = UNet2DConditionModel.from_pretrained(
    checkpoint, subfolder='unet', torch_dtype=torch.bfloat16)
vae = AutoencoderKL.from_pretrained(
    'madebyollin/sdxl-vae-fp16-fix',
    torch_dtype=torch.bfloat16,
)
pipe = DiffusionPipeline.from_pretrained(
    'stabilityai/stable-diffusion-xl-base-1.0', unet=unet, vae=vae, torch_dtype=torch.bfloat16)
pipe.to('cuda')

image = pipe(
    prompt,
    num_inference_steps=50,
    width=1024,
    height=1024,
).images[0]
image.save('demo.png')
```

You can see more details on [`docs/source/run_guides/run.md`](../../docs/source/run_guides/run.md#inference-with-diffusers).

## Results Example

#### stable_diffusion_xl_pokemon_blip

![example1](https://github.com/okotaku/diffengine/assets/24734142/dd04fb22-64fb-4c4f-8164-b8391d94abab)
